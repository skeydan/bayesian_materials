---
title: "Bayesian modeling and prediction for movies"
output: 
  html_document: 
    fig_height: 4
    highlight: pygments
    theme: spacelab
---

## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(gridExtra)
library(statsr)
library(BAS)
library(GGally)
```

### Load data

```{r load-data}
load("movies.Rdata")
```


* * *

## Part 1: Data

Our task is to predict the  audience score on Rotten Tomatoes from a set of movie features as well as metadata obtained from the IMDB and Rotten Tomatoes websites.
The data have been sampled at random, so the results will be generalizable; however, we will not be able to say anything about causality as this is an observation study (no random assignment was used).

Let's first inspect the dataset:

```{r}
print(nrow(movies))
head(movies)
```


* * *

## Part 2: Data manipulation

Before further exploring the data, let's first do some feature engineering...

```{r}
movies <- movies %>% mutate(
  feature_film = if_else(title_type == 'Feature Film', "yes", "no"),
  drama = if_else(genre == 'Drama', "yes", "no"),
  mpaa_rating_R = if_else(mpaa_rating == "R", "yes", "no"),
  oscar_season = if_else(thtr_rel_month %in% c(10,11,12), "yes", "no"),
  summer_season = if_else(thtr_rel_month %in% c(5,6,7,8), "yes", "no")
)
```

... and restrict the set of possible predictors to variables we think might be relevant:

```{r}
movies <- movies %>% select(
    audience_score, feature_film, drama, runtime, mpaa_rating_R, thtr_rel_year, oscar_season, summer_season,
    imdb_rating, imdb_num_votes, critics_score, best_pic_nom, best_pic_win, best_actor_win,
    best_actress_win, best_dir_win, top200_box
)
head(movies)
```


## Part 3: Exploratory data analysis

To get an idea about possible correlations, we plot audience_score against every hypothetical predictor:

```{r}
p1 <- ggplot(movies, aes(x = feature_film, y = audience_score)) + geom_boxplot()
p2 <- ggplot(movies, aes(x = drama, y = audience_score)) + geom_boxplot()
p3 <- ggplot(movies, aes(x = mpaa_rating_R, y = audience_score)) + geom_boxplot()
p4 <- ggplot(movies, aes(x = oscar_season, y = audience_score)) + geom_boxplot()
p5 <- ggplot(movies, aes(x = summer_season, y = audience_score)) + geom_boxplot()
p6 <- ggplot(movies, aes(x = runtime, y = audience_score)) + geom_point()
p7 <- ggplot(movies, aes(x = thtr_rel_year, y = audience_score)) + geom_point()
p8 <- ggplot(movies, aes(x = imdb_rating, y = audience_score)) + geom_point()
p9 <- ggplot(movies, aes(x = imdb_num_votes, y = audience_score)) + geom_point()
p10 <- ggplot(movies, aes(x = critics_score, y = audience_score)) + geom_point()
p11 <- ggplot(movies, aes(x = best_pic_nom, y = audience_score)) + geom_boxplot()
p12 <- ggplot(movies, aes(x = best_pic_win, y = audience_score)) + geom_boxplot()
p13 <- ggplot(movies, aes(x = best_actor_win, y = audience_score)) + geom_boxplot()
p14 <- ggplot(movies, aes(x = best_actress_win, y = audience_score)) + geom_boxplot()
p15 <- ggplot(movies, aes(x = best_dir_win, y = audience_score)) + geom_boxplot()
p16 <- ggplot(movies, aes(x = top200_box, y = audience_score)) + geom_boxplot()
grid.arrange(p1,p2,p3,p4)
grid.arrange(p5,p6,p7,p8)
grid.arrange(p9,p10,p11,p12)
grid.arrange(p13,p14,p15,p16)
```


From the plots, we think that feature_film, critics_score and imdb_rating might be interesting, as well as possibly best_pic_win and best_pic_nom.
In addition to the plots, let's inspect audience_score means and variances for the categorical variables, and inspect the intercorrelations for the numerical ones. The latter will also show us which predictors are intercorrelated and how much so.

First, the categorical variables:

```{r}
movies %>% group_by(feature_film) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(drama) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(mpaa_rating_R) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(oscar_season) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(summer_season) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(best_pic_nom) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(best_pic_win) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(best_actor_win) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(best_actress_win) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(best_dir_win) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```

```{r}
movies %>% group_by(top200_box) %>% summarise(mean = mean(audience_score), var = var(audience_score), cnt = n())
```


Now, the numerical predictors:

```{r}
movies_numeric <- movies %>% select(audience_score, runtime, thtr_rel_year, imdb_rating, imdb_num_votes, critics_score)
cor(movies_numeric, use = "complete.obs")
```

Not surprisingly, audience_score is highly correlated with imdb_rating and critics_score, but critics_score and audience_score are themselves highly correlated.


* * *

## Part 4: Modeling

Now, it's time to fit the models. We use a uniform prior over models and ZS-null prior for the coefficients.

```{r}
fit <- bas.lm(
  data = movies,
  formula = audience_score ~ . ,
  prior = "ZS-null",
  modelprior = uniform(),
  method = "BAS"
  )   
```


Let's directly look at the marginal posterior inclusion probabilities (pips) for the predictors.
All  predictors with pip > 0.5 would be included in the _median probability model_.

As we see, these are imdb_rating and critics_score (plus the intercept).

Interestingly, both are included, even though they are highly correlated (multicollinearity).


```{r}
plot(fit, which = 4) 
```



We can also get the pips printing the fit object returned.

Of course, we see the same result as in the plot.

```{r}
fit
```


Now, let's look at the best 5 models (posterior probability-wise):

```{r}
summary(fit)
```

So the highest posterior probability model, just like the median probability model, includes the intercept, imdb_rating, and critics_score.

Looking at the Bayes factors, and the posterior probabilities, for the other models, we see that there is just one model that works as well as the best one (row 2). This additionally includes runtime.

However, the correlation between audience_score and runtime is low, and this is anyway just the second best model, so there is no reason to include this additional predictor (uselessly inflating the number of parameters in the model.)

So we'll stick with predictors critics_score and imdb_rating.
Let's look at posterior probabilities and confidence intervals for these:

```{r}
plot(coef(fit), subset=c(9,11))
```


Both parameters are _very_ unlikely to be 0. We also see this from the confidence intervals.

```{r}
plot(confint(coef(fit), parm=9))
```

```{r}
plot(confint(coef(fit), parm=11))
```


* * *

## Part 5: Prediction

Now, let's try predicting audience_score for a new movie. 

We construct the movie by setting thtr_rel_year to 2016, choosing "yes" for all categorical variables and using the respective variable mean from the movies dataset for the numerical variables.

Because of the way the test data is constructed (using feature averages), we have every reason to expect very close predictions!

```{r}
movies_new <- data_frame(
  audience_score=mean(movies$audience_score),
  feature_film=factor("yes", levels = c("yes","no")),
  drama=factor("yes", levels = c("yes","no")),
  runtime=mean(na.omit(movies$runtime)),
  mpaa_rating_R=factor("yes", levels = c("yes","no")),
  thtr_rel_year=2016,
  oscar_season=factor("yes", levels = c("yes","no")),
  summer_season=factor("yes", levels = c("yes","no")),
  imdb_rating=mean(movies$imdb_rating),
  imdb_num_votes=mean(movies$imdb_num_votes),
  critics_score=mean(movies$critics_score),
  best_pic_win=factor("yes", levels = c("yes","no")),
  best_pic_nom=factor("yes", levels = c("yes","no")),
  best_actor_win=factor("yes", levels = c("yes","no")),
  best_actress_win=factor("yes", levels = c("yes","no")),
  best_dir_win=factor("yes", levels = c("yes","no")),
  top200_box=factor("yes", levels = c("yes","no"))
)
movies_new
  
```


We will compare predictions from

- the highest posterior probability model (HPM),
- the median probability model (MPM), and
- Bayesian model averaging (BMA).


First, the highest posterior probability model (HPM):

```{r}
HPM = predict(fit, movies_new, estimator="HPM")
HPM$fit[1]
```


As we see, the correct audience_score of 62.36252 was very well predicted (prediction: 62.3729).

We can double-check the features included in the model (we already know from above that these should be imdb_rating, critics_score, and the intercept):

```{r}
(fit$namesx[attr(HPM$fit, 'model') +1])
```


Next, the median probability model.

```{r}
MPM = predict(fit, movies_new, estimator="MPM")
MPM$fit[1]
```


Unsurprisingly, we get the same prediction, as the same predictors are included in the model:

```{r}
(fit$namesx[attr(MPM$fit, 'model') +1])
```


Finally, let's compare to Bayesian model averaging. This time, the prediction is a little bit farther away from the correct value.

```{r}
BMA = predict(fit, movies_new, estimator="BMA")
BMA$fit[1]
```


* * *

## Part 6: Conclusion

Although we managed to construct a powerful model, the basic research question is not answered - the basic research question, as I understand it, having been about intrinsic qualities of movies that influence their ratings.

The rating on IMDB, and the rating by critics, are not interesting predictors from this point of view.

For further research, I'd suggest including more relevant predictors, probably by using information from other websites (e.g., describing the plot, the acting ...) and/or using natural language processing to extract features from the written evaluations.



